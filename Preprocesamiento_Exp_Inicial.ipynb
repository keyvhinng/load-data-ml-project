{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar los datos en un dataframe\n",
    "pd.set_option('display.max_columns', 500)\n",
    "data = pd.read_csv('loan.csv',low_memory=False)\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las estadisticas descriptivas\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filtrar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se eliminan las columnas con mas de 90% de sus valores perdidos (nulos)\n",
    "print('Eliminando columnas con mas de 90% de valores perdidos...')\n",
    "print(f'Cantidad de columnas originales: {data.shape[1]}')\n",
    "cols_elim = [x for x in data.columns if data[x].isnull().sum()/data.count().max()>=0.10]\n",
    "print(f'Cantidad de columnas a eliminar: {len(cols_elim)}')\n",
    "data_clear = data.drop(columns=cols_elim,axis=1)\n",
    "print(f'Cantidad de columnas finales: {data_clear.shape[1]}')\n",
    "print('---------------')\n",
    "# Eliminar columnas cuyos nombres contengan id_\n",
    "id_lst=list(map(lambda x: 'id' in x,data_clear.columns))\n",
    "id_column_name_lst = list(data_clear.columns[id_lst])\n",
    "print('Eliminando columnas que contengan un id')\n",
    "print(f'Numero de columnas original: {len(data_clear.columns.tolist())}')\n",
    "data_clear.drop(columns=id_column_name_lst, axis=1,inplace=True)\n",
    "print(f'Numero de columnas filtrado: {len(data_clear.columns.tolist())}')\n",
    "print('---------------')\n",
    "# Eliminar columnas cuyos nombres contengan _d (fechas)\n",
    "id_lst=list(map(lambda x: '_d' in x,data_clear.columns))\n",
    "id_column_name_lst = list(data_clear.columns[id_lst])\n",
    "print('Eliminando columnas que contengan un dato que sea fecha (terminan en _d)')\n",
    "print(f'Numero de columnas original: {len(data_clear.columns.tolist())}')\n",
    "data_clear.drop(columns=id_column_name_lst, axis=1,inplace=True)\n",
    "print(f'Numero de columnas filtrado: {len(data_clear.columns.tolist())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar las columnas que tienen datos potencialmente categoricos y convertir sus valores de acuerdo al mismo\n",
    "print('Convirtiendo las categorias nominales en numericas mediante codificacion')\n",
    "cat_cols = data_clear.select_dtypes(include='object').columns.tolist()\n",
    "cat_cols.remove('loan_status')\n",
    "print(f'Se encontraron {len(cat_cols)} columnas por convertir.')\n",
    "cat_dict = dict()\n",
    "for col in cat_cols:\n",
    "    data_clear[col] = data_clear[col].astype('category')\n",
    "    cat_dict[col] = {i+1:c for i,c in enumerate(data_clear[col].cat.categories)}\n",
    "    data_clear[col] = data_clear[col].cat.codes+1\n",
    "print('Columnas convertidas con exito.')\n",
    "print('---------------')\n",
    "\n",
    "# Se mira la cantidad de categorias que tiene cada columna. Asimismo, se identifican aquellas que tienen una cantidad\n",
    "# muy alta de categorias distintas (>10% de la cantidad de registros de la columna)\n",
    "print(f'Eliminando columnas nominales que tienen demasiados valores distintos (limite: 10% del total de la columna)')\n",
    "debug = False\n",
    "cols_elim = []\n",
    "for keys,values in cat_dict.items():\n",
    "    if debug:\n",
    "        print(f'{keys} => {len(values)} de {data_clear.count().max()} ({round(100*len(values)/data_clear.count().max(),2)}%)')\n",
    "    if len(values)/data_clear.count().max() >= 0.1:\n",
    "        cols_elim.append(keys)\n",
    "print(f'Cantidad actual de columnas: {len(data_clear.columns.tolist())}')\n",
    "data_clear.drop(columns=cols_elim, axis=1, inplace=True)\n",
    "print(f'Cantidad nueva de columnas: {len(data_clear.columns.tolist())}')\n",
    "print('---------------')\n",
    "\n",
    "# Se rellenan los vacios con la mediana\n",
    "print('Rellenando los vacios con la mediana...')\n",
    "data_clear = data_clear.fillna(value=data_clear.median())\n",
    "data_clear = data_clear.reset_index(drop=True)\n",
    "print('Se rellenaron los vacios.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se explora la variable target (loan_status)\n",
    "import warnings\n",
    "def analisis_agregacion(dataframe,groupby_col,idx_opt):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "        #aggregation_function = {'id':{'cuenta':'count'},'loan_amnt':{'suma':'sum','contador':lambda x: int(sum(x)/data2.loc[x.index].loan_amnt.sum())}}\n",
    "        aggregation_function = {'loan_amnt':['count','sum']}\n",
    "        data_res=dataframe.groupby(groupby_col,as_index=idx_opt).agg(aggregation_function)\n",
    "    data_res['loan_amnt','porcentaje_cuenta'] = round(data_res['loan_amnt','count']/data_res['loan_amnt'].sum()[0]*100,2)\n",
    "    data_res['loan_amnt','tporcentaje_monto'] = round(data_res['loan_amnt','sum']/data_res['loan_amnt'].sum()[1]*100,2)\n",
    "    return data_res.sort_index(axis=1)\n",
    "\n",
    "print('Distribucion de la variable loan_status')\n",
    "print(analisis_agregacion(data_clear,'loan_status',True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se observa que no todos los valores son validos o aplicables al caso.\n",
    "# Por ello, se filtran los registros inconsistentes o no aplicables para analisis:\n",
    "#    Current -> No aplicable porque son préstamos dentro de su periodo de pago.\n",
    "#    Does not meet the credit policy (...) -> Inconsistente. No se sabe si es un error o un valor correcto.\n",
    "#    Issued -> No aplicable porque son créditos recién emitidos.\n",
    "#    In grace period -> No aplicable porque son créditos en periodo de gracia.\n",
    "print('Eliminando registros inconsistentes o no aplicables para analisis.')\n",
    "estados_eliminar = {'In Grace Period','Current','Does not meet the credit policy. Status:Fully Paid', 'Does not meet the credit policy. Status:Charged Off', 'Issued'}\n",
    "data_clear_final = data_clear\n",
    "for estado in estados_eliminar:\n",
    "    data_clear_final.drop(data_clear_final[data_clear_final.loan_status==estado].index,inplace=True)\n",
    "    print(f'Eliminado {estado}. Quedan: {data_clear_final.count()[0]}')\n",
    "print('Se eliminaron registros no aplicables')\n",
    "print('Nueva distribucion:')\n",
    "print('==============================================================================================')\n",
    "print(analisis_agregacion(data_clear_final,'loan_status',True))\n",
    "print('==============================================================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se genera la variable de destino (target). En este caso, el estado de prestamo pagado será la clase positiva (+1)\n",
    "# y cualquier otro estado será negativo (-1)\n",
    "print('Creando columna de clase...')\n",
    "data_clear_final['class'] = 2*(data_clear_final['loan_status'] == 'Fully Paid')-1\n",
    "print('==============================================================================================')\n",
    "print('Distribucion de la nueva columna:')\n",
    "print('')\n",
    "print(analisis_agregacion(data_clear_final,['class','loan_status'],True))\n",
    "\n",
    "print('==============================================================================================')\n",
    "print('Se separa el set X del Y')\n",
    "X = data_clear_final[data_clear_final.drop(columns=['loan_status','class'],axis=1).columns.tolist()]\n",
    "y = data_clear_final['class']\n",
    "print(f'Las dimensiones de X son: {X.shape}')\n",
    "print(f'Las dimensiones de y son: {y.shape}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparar Datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparacion experimental\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Separar el conjunto de entrenamiento/validacion del conjunto de prueba\n",
    "print('Separando el conjunto de entrenamiento del de pruebas')\n",
    "print(f'El tamaño original de X es {len(X)}')\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.20, random_state=40)\n",
    "print(f'El tamaño del set de entrenamiento es: {len(X_trainval)}')\n",
    "print(f'El tamaño del set de pruebas es: {len(X_test)}')\n",
    "print(f'El tamaño del set entrenamiento + pruebas es {len(X_trainval+X_test)}')\n",
    "\n",
    "# Normalizar el conjunto de entrenamiento/valdiacion\n",
    "print('')\n",
    "print('Ejecutando normalizacion...')\n",
    "scaler = StandardScaler().fit(X_trainval)\n",
    "X_trainval_scaled = scaler.transform(X_trainval)\n",
    "print('Set de pruebas normalizado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seleccion de Caracteristicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sea conjunto_n_caracteristicas = {10,20,30} el conjunto de dimensiones\n",
    "# Sea conjunto_algoritmos = {Logistic_Regresion, RandomForest, SVM}\n",
    "# Para cada n_caracteristicas en conjunto_n_caracteristicas:\n",
    "#   Para cada algoritmo en conjunto_algoritmo:\n",
    "#      conjunto_caracteristas[algoritmo] = reducirCaracteristicas(n_dim,algoritmo)\n",
    "#   caracteristicas_comunes[n_caracteristicas] = unirCaracteristicas(conjunto_caracteristicas)\n",
    "#   scores[n_caracteristicas] = entrenarClasificador(caracteristicas_comunes)\n",
    "# mejor_conjunto_caracteristicas = encontrarMejorConjunto(caracteristicas_comunes, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso de pipes para entrenar modelo y seleccionar caracteristicas con GridSearchCV\n",
    "modelo_logistico = LogisticRegression()\n",
    "selector_logistico = RFE(estimator=modelo_logistico)\n",
    "#selector_logistico = selector_logistico.fit(X_trainval_scaled,y_trainval)\n",
    "pipe = Pipeline(steps=[('reducir_dim', selector_logistico), ('logistic', modelo_logistico)])\n",
    "\n",
    "n_caracteristicas = list(range(10,len(X_trainval.columns)-5,10))\n",
    "c_valores = [0.1]\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        'reducir_dim__n_features_to_select': n_caracteristicas,\n",
    "        'logistic__C': c_valores\n",
    "    }\n",
    "]\n",
    "\n",
    "grid = GridSearchCV(pipe, cv=10, n_jobs=10, param_grid=param_grid, return_train_score=False, verbose=10)\n",
    "grid.fit(X_trainval,y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_params_['reducir_dim__n_features_to_select']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo_logistico = LogisticRegression(C=0.1)\n",
    "selector_logistico = RFE(estimator=modelo_logistico,n_features_to_select=30)\n",
    "selector_logistico.fit(X_trainval,y_trainval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_logistico.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
